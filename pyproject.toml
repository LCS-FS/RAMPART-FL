[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "RAMPART-FLIDS"
version = "1.0.0"
description = "Reinforcement-based Adaptive Multi-Criteria Participant Selection for Federated Learning in Intrusion Detection for Edge Devices"
license = "Apache-2.0"
dependencies = [
    "flwr[simulation]>=1.15.2",
    "flwr-datasets[vision]>=0.5.0",
    "torch==2.5.1",
    "torchvision==0.20.1",
]

[tool.hatch.build.targets.wheel]
packages = ["."]

[tool.flwr.app]
publisher = "flower"

[tool.flwr.app.components]
serverapp = "src.server_app:app"
clientapp = "src.client_app:app"

[tool.flwr.app.config]
num-server-rounds = 400
fraction-fit = 0.5
local-epochs = 10
# RL Agent Hyperparameters
rl-learning-rate = 0.01
rl-discount-factor = 0.9
rl-temperature-initial = 1.0
rl-temperature-decay = 0.995
rl-temperature-min = 0.01
rl-history-length = 3
rl-recency-window = 5 # For participation tracking
rl-fairness-penalty-factor = 0.01 # Penalty for over-selection. Decreased from 0.1
rl-fairness-participation-threshold-factor = 0.5 # Factor of recency_window to define threshold
rl-w-f1 = 0.8 # Reward weight for F1-score change
rl-w-precision = 0.25 # Reward weight for precision change
rl-w-recall = 0.25 # Reward weight for recall change
#rl-num-clients-to-select-fallback = 5 # Fallback/cap for number of clients RL selects
rl-prob-threshold-factor = 1.0 # Factor for dynamic probability-based RL client selection
rl-resource-cost-factor = 0.02 # Factor to scale total selected cores into a cost
rl-ablation-mode = "multi-criteria" # "multi-criteria", "performance-only", "resource-only", or "fairness-only"

[tool.flwr.federations]
default = "local-simulation"

[tool.flwr.federations.local-simulation]
options.num-supernodes = 16

[tool.flwr.federations.local-deployment]
address = "127.0.0.1:9093"
insecure = true
